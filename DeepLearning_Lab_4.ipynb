{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY1vsnKr3m6cNusR5btVaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atharv24-Atreus/Deep-Learning-Lab/blob/main/DeepLearning_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8xmo6bza967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras import utils\n",
        "import itertools\n",
        "\n",
        "# Load dataset\n",
        "data = np.load('ORL_faces.zip')\n",
        "\n",
        "# Extract training and testing data from the loaded data\n",
        "# The keys 'trainX' and 'testX' are assumed to exist in the loaded data\n",
        "x_train = data['trainX']  # Extract training data\n",
        "x_test = data['testX']   # Extract testing data\n",
        "\n",
        "\n",
        "# Normalize every image by dividing by 255\n",
        "x_train = np.array(x_train, dtype='float32') / 255\n",
        "\n",
        "x_test = np.array(x_test, dtype='float32') / 255\n",
        "\n",
        "# Load the labels of images\n",
        "y_train = data['trainY']\n",
        "y_test = data['testY']\n",
        "\n",
        "# Show the train and test data format\n",
        "print('x_train : {}'.format(x_train[:]))\n",
        "print('Y-train shape: {}'.format(y_train))\n",
        "print('x_test shape: {}'.format(x_test.shape))\n",
        "\n",
        "# Split train data further into train and validation sets\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train, y_train, test_size=0.05, random_state=1234\n",
        ")\n",
        "\n",
        "# Image dimensions and shape\n",
        "im_rows = 112\n",
        "im_cols = 92\n",
        "batch_size = 512\n",
        "im_shape = (im_rows, im_cols, 1)\n",
        "\n",
        "# Reshape the image data to match the CNN input expectations\n",
        "x_train = x_train.reshape(-1, *im_shape)\n",
        "x_test = x_test.reshape(-1, *im_shape)\n",
        "x_valid = x_valid.reshape(-1, *im_shape)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes=20)\n",
        "y_valid = to_categorical(y_valid, num_classes=20)\n",
        "y_test = to_categorical(y_test, num_classes=20)\n",
        "\n",
        "# Display the shape of the datasets\n",
        "print('x_train shape: {}'.format(x_train.shape))\n",
        "print('y_train shape: {}'.format(y_train.shape))\n",
        "print('x_test shape: {}'.format(x_test.shape))\n",
        "\n",
        "print(tf.size(x_train))\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(filters=36, kernel_size=7, activation='relu', input_shape=im_shape),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Conv2D(filters=54, kernel_size=5, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(2024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='softmax')  # 20 classes for classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "# Train the model\n",
        "history = cnn_model.fit(\n",
        "    np.array(x_train), np.array(y_train), batch_size=64,\n",
        "    epochs=5, verbose=2,\n",
        "    validation_data=(np.array(x_valid), np.array(y_valid))\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "scor = cnn_model.evaluate(np.array(x_test), np.array(y_test), verbose=0)\n",
        "\n",
        "print('Test loss: {:.4f}'.format(scor[0]))\n",
        "print('Test accuracy: {:.4f}'.format(scor[1]))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "7P6L0RXjbq2w",
        "outputId": "12ff8192-43cb-40fe-8441-83e2ff157b8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'trainX is not a file in the archive'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4c3ae6f86240>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Extract training and testing data from the loaded data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# The keys 'trainX' and 'testX' are assumed to exist in the loaded data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainX'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'testX'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Extract testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} is not a file in the archive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'trainX is not a file in the archive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = data['trainY']\n",
        "y_test = data['testY']\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=20)\n",
        "y_valid = to_categorical(y_valid, num_classes=20)\n",
        "y_train = data['trainY']\n",
        "y_test = data['testY']\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=20)\n",
        "y_test = to_categorical(y_test, num_classes=20)\n",
        "# Assuming y_valid is defined somewhere. If not, you will need to define it\n",
        "# y_valid = to_categorical(y_valid, num_classes=20)\n",
        "\n",
        "\n",
        "#This loss function is specifically designed to work with\n",
        "#one-hot encoded labels. If you provide integer labels, it will throw an error or produce incorrect results."
      ],
      "metadata": {
        "id": "aWsGGIrociZk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5e715f8c-b143-47de-de6f-55894d9e58b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-00eccb22a33e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'testY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqLSzBpljvk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}